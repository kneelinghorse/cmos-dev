# pack.yaml - Metadata for the "Technical Research" Domain Pack
name: "Build.TechnicalResearch.v1"
version: "1.0.0"
displayName: "Technical Research Mission"
description: "A mission to answer specific, technical questions to inform an engineering build. It focuses on identifying algorithms, patterns, performance metrics, and constraints, producing actionable insights for an implementation mission." #
author: "Your Name/Team"
schema: "./schemas/TechnicalResearch.v1.json"

---

# Mission File: R#.#_research-topic.yaml

missionId: "TR-20251004-001" # Technical Research

objective: "To answer specific technical questions and produce a structured set of actionable insights and evidence for a subsequent build mission."

context: |
  This mission is a focused research spike.
  The goal is not broad exploration, but to gather the specific, necessary data to de-risk and accelerate a planned implementation.
  The findings from this mission will form the 'Research Foundation' of a 'Build.Implementation' mission.

successCriteria:
  - "All research questions in the 'domainFields' are answered."
  - "The 'buildImplications' section is populated with clear, actionable recommendations."
  - "All key findings are supported by cited sources in the 'evidenceCollection'."
  - "Contradictions or areas of uncertainty are explicitly documented."

deliverables:
  - "This completed and validated mission file."
  - "A structured 'buildImplications' object ready for use in a build mission."

domainFields:
  type: "Build.TechnicalResearch.v1"

  # Define the specific questions for this research spike.
  researchQuestions:
    - "What is the most performant Python library for in-memory graph traversal on datasets up to 1GB?"
    - "What are the common implementation patterns and edge cases for this library?"
    - "What are the performance benchmarks for insertion and query operations?"
  
  # A structured log of key findings from the investigation.
  keyFindings:
    - category: "Library Comparison"
      details: "NetworkX is feature-rich but slower. igraph is significantly faster for our target dataset size."
    - category: "Performance Metrics"
      details: "igraph query time for 1M-edge graph averages <50ms. Insertion is O(log n)."
    - category: "Implementation Pattern"
      details: "Best practice is to build the graph from an edge list in bulk, not node by node."

  # Note any conflicting information or areas needing further validation.
  contradictionsAndUncertainties: "Some blog posts claim igraph has a steeper learning curve, but official documentation seems clear. This should be validated with a small PoC."

  # The primary, structured output for the next mission.
  buildImplications:
    recommendedAlgorithm: "Use the 'igraph' library for all graph operations."
    performanceTarget: "All graph queries must complete in under 100ms on the benchmark dataset."
    constraintToRespect: "The implementation must use the bulk-loading method for graph creation to ensure efficient startup."
    
  # Enforces the 'Evidence Chain' pattern.
  evidenceCollection:
    - type: "Documentation"
      reference: "igraph Official API Docs, v0.10"
      key_insight: "API for bulk-loading from an edge list."
      confidence: "High"
    - type: "Blog / Benchmark"
      reference: "PyGraphBenchy, 'A 2024 Performance Comparison of Python Graph Libraries'"
      key_insight: "igraph outperformed NetworkX by 5-10x on queries for graphs >100k edges."
      confidence: "Medium"